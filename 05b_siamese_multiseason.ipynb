{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Player Replacement Finder - Siamese Neural Network\n",
                "## All Positions Version (Per-90 Normalized)\n",
                "\n",
                "Finds similar players across all positions using multi-season data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install torch torchvision tqdm mplsoccer -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import random\n",
                "import json\n",
                "from pathlib import Path\n",
                "from typing import Optional, Dict, List, Tuple, Any\n",
                "import pickle\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from mplsoccer import Radar, FontManager, grid\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore', category=FutureWarning)\n",
                "\n",
                "# ==================== CONFIGURATION ====================\n",
                "SEED = 42\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed_all(SEED)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "    torch.backends.cudnn.benchmark = False\n",
                "\n",
                "# Training hyperparameters\n",
                "BATCH_SIZE = 64\n",
                "LEARNING_RATE = 0.001\n",
                "WEIGHT_DECAY = 1e-5\n",
                "EPOCHS = 50\n",
                "EMB_DIM = 64\n",
                "TRAIN_PAIRS = 50000\n",
                "VAL_PAIRS = 10000\n",
                "VAL_SPLIT = 0.2\n",
                "PATIENCE = 10\n",
                "\n",
                "# Device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "print(f\"Random seed: {SEED}\")\n",
                "\n",
                "# Paths\n",
                "SIAMESE_DIR = Path(\".\")\n",
                "DATA_DIR = SIAMESE_DIR / \"data\"\n",
                "MODELS_DIR = SIAMESE_DIR / \"models\"\n",
                "OUTPUTS_DIR = SIAMESE_DIR / \"outputs\"\n",
                "\n",
                "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "allpos_file = DATA_DIR / \"players_allpos_multiseason.csv\"\n",
                "forwards_file = DATA_DIR / \"forwards_multiseason.csv\"\n",
                "\n",
                "if allpos_file.exists():\n",
                "    df = pd.read_csv(allpos_file)\n",
                "    SOURCE = \"allpos_multiseason\"\n",
                "    print(f\"ALL POSITIONS: {len(df)} players\")\n",
                "elif forwards_file.exists():\n",
                "    df = pd.read_csv(forwards_file)\n",
                "    SOURCE = \"forwards_multiseason\"\n",
                "    print(f\"Forwards only: {len(df)} players\")\n",
                "else:\n",
                "    raise FileNotFoundError(\"No data found! Run 00_process_allpos.ipynb first.\")\n",
                "\n",
                "print(f\"Columns: {len(df.columns)}\")\n",
                "if 'Pos' in df.columns:\n",
                "    print(f\"\\nPosition breakdown:\")\n",
                "    print(df['Pos'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_col(names: List[str]) -> Optional[str]:\n",
                "    for name in names:\n",
                "        for col in df.columns:\n",
                "            if str(col).lower() == name.lower():\n",
                "                return col\n",
                "    return None\n",
                "\n",
                "PLAYER_COL = find_col(['player', 'Player', 'name'])\n",
                "TEAM_COL = find_col(['squad', 'Squad', 'team', 'Team'])\n",
                "AGE_COL = find_col(['age', 'Age'])\n",
                "POS_COL = find_col(['pos', 'Pos', 'position'])\n",
                "NINETIES_COL = find_col(['90s', '90s_x', '90s_y'])\n",
                "\n",
                "print(f\"Player: {PLAYER_COL}, Team: {TEAM_COL}, Age: {AGE_COL}, Pos: {POS_COL}, 90s: {NINETIES_COL}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_features(filepath: Path) -> Tuple[List[str], Dict[str, str]]:\n",
                "    features = []\n",
                "    feature_names = {}\n",
                "    with open(filepath, 'r', encoding='utf-8') as f:\n",
                "        for line in f:\n",
                "            line = line.strip()\n",
                "            if not line or line.startswith('#'):\n",
                "                continue\n",
                "            if '#' in line:\n",
                "                parts = line.split('#', 1)\n",
                "                raw_name = parts[0].strip()\n",
                "                readable_name = parts[1].strip()\n",
                "                if raw_name and readable_name:\n",
                "                    features.append(raw_name)\n",
                "                    feature_names[raw_name] = readable_name\n",
                "    return features, feature_names\n",
                "\n",
                "allpos_features = DATA_DIR / 'clustering_features_allpos.txt'\n",
                "multiseason_features = DATA_DIR / 'clustering_features_multiseason.txt'\n",
                "\n",
                "if allpos_features.exists():\n",
                "    FEATURES, FEATURE_NAMES = load_features(allpos_features)\n",
                "elif multiseason_features.exists():\n",
                "    FEATURES, FEATURE_NAMES = load_features(multiseason_features)\n",
                "else:\n",
                "    raise FileNotFoundError(\"No features file found!\")\n",
                "\n",
                "FEATURES = [f for f in FEATURES if f in df.columns]\n",
                "print(f\"Loaded {len(FEATURES)} features\")\n",
                "\n",
                "RATE_FEATURES = [\n",
                "    'Per 90 Minutes', 'Per 90 Minutes.1', 'Per 90 Minutes.2', 'Per 90 Minutes.3',\n",
                "    'Per 90 Minutes.4', 'Per 90 Minutes.5', 'Per 90 Minutes.6', 'Per 90 Minutes.7',\n",
                "    'Per 90 Minutes.8', 'Per 90 Minutes.9',\n",
                "    'Standard.2', 'Standard.3', 'Standard.4', 'Standard.5', 'Standard.6', 'Standard.11',\n",
                "    'Total.2', 'Short.2', 'Medium.2', 'Long.2',\n",
                "    'Take-Ons.2', 'Take-Ons.4', 'Aerial Duels.2', 'Challenges.2', 'Expected.4'\n",
                "]\n",
                "\n",
                "def get_readable_name(feature: str) -> str:\n",
                "    return FEATURE_NAMES.get(feature, feature)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_per90 = df.copy()\n",
                "if NINETIES_COL and NINETIES_COL in df.columns:\n",
                "    nineties = df[NINETIES_COL].replace(0, np.nan)\n",
                "    for feat in FEATURES:\n",
                "        if feat not in RATE_FEATURES and feat != '90s':\n",
                "            df_per90[feat] = df[feat] / nineties\n",
                "    df_per90[FEATURES] = df_per90[FEATURES].fillna(0)\n",
                "    print(\"Converted to per-90 format\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df_per90[FEATURES].fillna(0).values\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "print(f\"Feature matrix: {X_scaled.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Ground Truth Similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Computing similarity matrix...\")\n",
                "ground_truth = cosine_similarity(X_scaled)\n",
                "ground_truth = (ground_truth + 1) / 2\n",
                "print(f\"Shape: {ground_truth.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PairDataset(Dataset):\n",
                "    def __init__(self, features, sim_matrix, indices, n_pairs=10000, seed=SEED):\n",
                "        self.features = torch.FloatTensor(features)\n",
                "        self.sim = sim_matrix\n",
                "        self.indices = indices\n",
                "        self.n = len(indices)\n",
                "        rng = np.random.RandomState(seed)\n",
                "        self.pairs = []\n",
                "        self.labels = []\n",
                "        for _ in range(n_pairs):\n",
                "            pair_idx = rng.choice(self.n, 2, replace=False)\n",
                "            i, j = self.indices[pair_idx[0]], self.indices[pair_idx[1]]\n",
                "            self.pairs.append((i, j))\n",
                "            self.labels.append(sim_matrix[i, j])\n",
                "    def __len__(self): return len(self.pairs)\n",
                "    def __getitem__(self, idx):\n",
                "        i, j = self.pairs[idx]\n",
                "        return self.features[i], self.features[j], torch.FloatTensor([self.labels[idx]])\n",
                "\n",
                "all_indices = np.arange(len(X_scaled))\n",
                "train_indices, val_indices = train_test_split(all_indices, test_size=VAL_SPLIT, random_state=SEED)\n",
                "print(f\"Train players: {len(train_indices)}, Val players: {len(val_indices)}\")\n",
                "\n",
                "train_data = PairDataset(X_scaled, ground_truth, train_indices, TRAIN_PAIRS, seed=SEED)\n",
                "val_data = PairDataset(X_scaled, ground_truth, val_indices, VAL_PAIRS, seed=SEED + 1)\n",
                "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
                "print(f\"Train pairs: {len(train_data)}, Val pairs: {len(val_data)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SiameseNet(nn.Module):\n",
                "    def __init__(self, in_dim, emb_dim=64):\n",
                "        super().__init__()\n",
                "        self.enc = nn.Sequential(\n",
                "            nn.Linear(in_dim, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.3),\n",
                "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),\n",
                "            nn.Linear(128, emb_dim)\n",
                "        )\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(emb_dim * 3, 128), nn.ReLU(), nn.Dropout(0.2),\n",
                "            nn.Linear(128, 64), nn.ReLU(),\n",
                "            nn.Linear(64, 1), nn.Sigmoid()\n",
                "        )\n",
                "    def forward(self, x1, x2):\n",
                "        e1, e2 = self.enc(x1), self.enc(x2)\n",
                "        combined = torch.cat([torch.abs(e1 - e2), e1 * e2, (e1 + e2) / 2], dim=1)\n",
                "        return self.fc(combined), e1, e2\n",
                "    def embed(self, x):\n",
                "        self.eval()\n",
                "        with torch.no_grad(): return self.enc(x).cpu().numpy()\n",
                "\n",
                "model = SiameseNet(len(FEATURES), EMB_DIM).to(device)\n",
                "print(f\"Network: {len(FEATURES)} features -> {EMB_DIM}D embeddings\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "criterion = nn.MSELoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
                "\n",
                "best_loss = float('inf')\n",
                "patience_counter = 0\n",
                "train_losses, val_losses = [], []\n",
                "\n",
                "print(f\"Training {EPOCHS} epochs (patience: {PATIENCE})...\\n\")\n",
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    t_loss = 0.0\n",
                "    for x1, x2, y in train_loader:\n",
                "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        pred, _, _ = model(x1, x2)\n",
                "        loss = criterion(pred, y)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        t_loss += loss.item()\n",
                "    t_loss /= len(train_loader)\n",
                "    \n",
                "    model.eval()\n",
                "    v_loss = 0.0\n",
                "    with torch.no_grad():\n",
                "        for x1, x2, y in val_loader:\n",
                "            x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
                "            pred, _, _ = model(x1, x2)\n",
                "            v_loss += criterion(pred, y).item()\n",
                "    v_loss /= len(val_loader)\n",
                "    \n",
                "    train_losses.append(t_loss)\n",
                "    val_losses.append(v_loss)\n",
                "    scheduler.step(v_loss)\n",
                "    \n",
                "    if v_loss < best_loss:\n",
                "        best_loss = v_loss\n",
                "        patience_counter = 0\n",
                "        torch.save(model.state_dict(), MODELS_DIR / 'siamese_allpos_best.pth')\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        if patience_counter >= PATIENCE:\n",
                "            print(f\"\\nEarly stopping at epoch {epoch + 1}!\")\n",
                "            break\n",
                "    \n",
                "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
                "        print(f\"Epoch {epoch + 1:3d} | Train: {t_loss:.4f} | Val: {v_loss:.4f}\")\n",
                "\n",
                "print(f\"\\nBest validation loss: {best_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(train_losses, label='Train')\n",
                "plt.plot(val_losses, label='Val')\n",
                "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
                "plt.title(f'Training Curve - All Positions ({SOURCE})')\n",
                "plt.legend(); plt.grid(alpha=0.3)\n",
                "plt.savefig(OUTPUTS_DIR / 'training_curve_allpos.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    model.load_state_dict(torch.load(MODELS_DIR / 'siamese_allpos_best.pth', weights_only=True))\n",
                "except TypeError:\n",
                "    model.load_state_dict(torch.load(MODELS_DIR / 'siamese_allpos_best.pth'))\n",
                "\n",
                "embeddings = model.embed(torch.FloatTensor(X_scaled).to(device))\n",
                "print(f\"Embeddings: {embeddings.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Replacement Finder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ReplacementFinder:\n",
                "    def __init__(self, df, df_per90, emb, player_col, team_col, age_col, pos_col, features, X_scaled):\n",
                "        self.df = df\n",
                "        self.df_per90 = df_per90\n",
                "        self.player_col = player_col\n",
                "        self.team_col = team_col\n",
                "        self.age_col = age_col\n",
                "        self.pos_col = pos_col\n",
                "        self.features = features\n",
                "        self.X_scaled = X_scaled\n",
                "        self.idx = {n: i for i, n in enumerate(df[player_col])}\n",
                "        self.sim = (cosine_similarity(emb) + 1) / 2\n",
                "    \n",
                "    def find(self, name, n=10, exclude_team=True, max_age=None, position=None):\n",
                "        idx = self._get_idx(name)\n",
                "        if idx is None:\n",
                "            print(f\"'{name}' not found\")\n",
                "            return None\n",
                "        res = self.df.copy()\n",
                "        res['similarity'] = self.sim[idx]\n",
                "        actual_name = self.df.iloc[idx][self.player_col]\n",
                "        res = res[res[self.player_col] != actual_name]\n",
                "        if exclude_team:\n",
                "            team = self.df.iloc[idx][self.team_col]\n",
                "            res = res[res[self.team_col] != team]\n",
                "        if max_age and self.age_col:\n",
                "            res = res[res[self.age_col] <= max_age]\n",
                "        if position and self.pos_col:\n",
                "            res = res[res[self.pos_col].str.contains(position, na=False)]\n",
                "        return res.nlargest(n, 'similarity')\n",
                "    \n",
                "    def _get_idx(self, name):\n",
                "        return next((i for p, i in self.idx.items() if name.lower() in p.lower()), None)\n",
                "\n",
                "finder = ReplacementFinder(df, df_per90, embeddings, PLAYER_COL, TEAM_COL, AGE_COL, POS_COL, FEATURES, X_scaled)\n",
                "print(\"Finder ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Visualizations (mplsoccer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Radar chart parameters for mplsoccer\n",
                "RADAR_STATS = {\n",
                "    'Per 90 Minutes': 'Goals',\n",
                "    'Per 90 Minutes.1': 'Assists',\n",
                "    'Per 90 Minutes.5': 'xG',\n",
                "    'Per 90 Minutes.6': 'xAG',\n",
                "    'Take-Ons.1': 'Dribbles',\n",
                "    'Carries.3': 'Prog Carries',\n",
                "    'KP': 'Key Passes',\n",
                "    'PrgP': 'Prog Passes',\n",
                "    'Tackles': 'Tackles',\n",
                "    'Int': 'Interceptions'\n",
                "}\n",
                "\n",
                "RADAR_FEATURES = [f for f in RADAR_STATS.keys() if f in FEATURES]\n",
                "RADAR_LABELS = [RADAR_STATS[f] for f in RADAR_FEATURES]\n",
                "\n",
                "# Compute min/max ranges for each stat (5th and 95th percentile)\n",
                "RADAR_LOW = [np.percentile(df_per90[f].dropna(), 5) for f in RADAR_FEATURES]\n",
                "RADAR_HIGH = [np.percentile(df_per90[f].dropna(), 95) for f in RADAR_FEATURES]\n",
                "\n",
                "print(f\"Radar chart: {len(RADAR_FEATURES)} stats\")\n",
                "for i, label in enumerate(RADAR_LABELS):\n",
                "    print(f\"  {label}: {RADAR_LOW[i]:.2f} - {RADAR_HIGH[i]:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def radar_chart(player1: str, player2: str, save_path: Optional[str] = None):\n",
                "    \"\"\"Create a professional radar chart comparing two players using mplsoccer.\"\"\"\n",
                "    idx1 = finder._get_idx(player1)\n",
                "    idx2 = finder._get_idx(player2)\n",
                "    \n",
                "    if idx1 is None or idx2 is None:\n",
                "        print(\"Player not found\")\n",
                "        return None\n",
                "    \n",
                "    name1 = df.iloc[idx1][PLAYER_COL]\n",
                "    name2 = df.iloc[idx2][PLAYER_COL]\n",
                "    team1 = df.iloc[idx1][TEAM_COL]\n",
                "    team2 = df.iloc[idx2][TEAM_COL]\n",
                "    pos1 = df.iloc[idx1][POS_COL] if POS_COL else ''\n",
                "    pos2 = df.iloc[idx2][POS_COL] if POS_COL else ''\n",
                "    similarity = finder.sim[idx1, idx2]\n",
                "    \n",
                "    # Get values for radar features\n",
                "    v1 = df_per90.iloc[idx1][RADAR_FEATURES].values.tolist()\n",
                "    v2 = df_per90.iloc[idx2][RADAR_FEATURES].values.tolist()\n",
                "    \n",
                "    # Create mplsoccer Radar\n",
                "    radar = Radar(\n",
                "        params=RADAR_LABELS,\n",
                "        min_range=RADAR_LOW,\n",
                "        max_range=RADAR_HIGH,\n",
                "        round_int=[False] * len(RADAR_LABELS),\n",
                "        num_rings=4,\n",
                "        ring_width=1,\n",
                "        center_circle_radius=1\n",
                "    )\n",
                "    \n",
                "    # Create figure with grid\n",
                "    fig, axs = grid(figheight=14, grid_height=0.915, title_height=0.06,\n",
                "                    endnote_height=0.025, title_space=0, endnote_space=0,\n",
                "                    grid_key='radar', axis=False)\n",
                "    \n",
                "    # Draw radar comparison\n",
                "    radar.setup_axis(ax=axs['radar'])\n",
                "    rings_inner = radar.draw_circles(ax=axs['radar'], facecolor='#f0f0f0', edgecolor='#cdcdcd')\n",
                "    radar_output = radar.draw_radar_compare(\n",
                "        v1, v2, ax=axs['radar'],\n",
                "        kwargs_radar={'facecolor': '#006B3F', 'alpha': 0.6},\n",
                "        kwargs_compare={'facecolor': '#CE1126', 'alpha': 0.6}\n",
                "    )\n",
                "    radar_poly, radar_poly2, vertices1, vertices2 = radar_output\n",
                "    range_labels = radar.draw_range_labels(ax=axs['radar'], fontsize=10)\n",
                "    param_labels = radar.draw_param_labels(ax=axs['radar'], fontsize=12)\n",
                "    \n",
                "    # Title\n",
                "    title1 = axs['title'].text(0.01, 0.65, name1, fontsize=20, fontweight='bold',\n",
                "                               ha='left', va='center', color='#006B3F')\n",
                "    title2 = axs['title'].text(0.01, 0.25, f\"{team1} | {pos1}\", fontsize=14,\n",
                "                               ha='left', va='center', color='#333333')\n",
                "    title3 = axs['title'].text(0.99, 0.65, name2, fontsize=20, fontweight='bold',\n",
                "                               ha='right', va='center', color='#CE1126')\n",
                "    title4 = axs['title'].text(0.99, 0.25, f\"{team2} | {pos2}\", fontsize=14,\n",
                "                               ha='right', va='center', color='#333333')\n",
                "    title5 = axs['title'].text(0.5, 0.5, f\"Similarity: {similarity:.1%}\", fontsize=16,\n",
                "                               ha='center', va='center', color='#555555', fontweight='bold')\n",
                "    \n",
                "    # Endnote\n",
                "    axs['endnote'].text(0.5, 0.5, 'Data: FBref | Player Replacement Finder',\n",
                "                        fontsize=10, ha='center', va='center', color='#666666')\n",
                "    \n",
                "    if save_path:\n",
                "        fig.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
                "    \n",
                "    plt.show()\n",
                "    \n",
                "    return {\n",
                "        'player1': {'name': name1, 'team': team1, 'position': pos1, 'values': v1},\n",
                "        'player2': {'name': name2, 'team': team2, 'position': pos2, 'values': v2},\n",
                "        'labels': RADAR_LABELS,\n",
                "        'similarity': float(similarity)\n",
                "    }\n",
                "\n",
                "print(\"radar_chart() ready - using mplsoccer!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pizza_chart(player: str, save_path: Optional[str] = None):\n",
                "    \"\"\"Create a pizza chart (single player percentile rankings) using mplsoccer.\"\"\"\n",
                "    from mplsoccer import PyPizza\n",
                "    \n",
                "    idx = finder._get_idx(player)\n",
                "    if idx is None:\n",
                "        print(\"Player not found\")\n",
                "        return None\n",
                "    \n",
                "    name = df.iloc[idx][PLAYER_COL]\n",
                "    team = df.iloc[idx][TEAM_COL]\n",
                "    pos = df.iloc[idx][POS_COL] if POS_COL else ''\n",
                "    \n",
                "    # Calculate percentiles for each stat\n",
                "    values = df_per90.iloc[idx][RADAR_FEATURES].values\n",
                "    percentiles = []\n",
                "    for i, feat in enumerate(RADAR_FEATURES):\n",
                "        all_vals = df_per90[feat].dropna().values\n",
                "        pct = (np.sum(all_vals < values[i]) / len(all_vals)) * 100\n",
                "        percentiles.append(int(round(pct)))\n",
                "    \n",
                "    # Create pizza chart\n",
                "    baker = PyPizza(\n",
                "        params=RADAR_LABELS,\n",
                "        background_color=\"#f4f4f4\",\n",
                "        straight_line_color=\"#000000\",\n",
                "        straight_line_lw=1,\n",
                "        last_circle_color=\"#000000\",\n",
                "        last_circle_lw=1,\n",
                "        other_circle_lw=0,\n",
                "        inner_circle_size=20\n",
                "    )\n",
                "    \n",
                "    fig, ax = baker.make_pizza(\n",
                "        percentiles,\n",
                "        figsize=(10, 10),\n",
                "        color_blank_space=\"same\",\n",
                "        slice_colors=[\"#006B3F\"] * len(RADAR_LABELS),\n",
                "        value_colors=[\"#ffffff\"] * len(RADAR_LABELS),\n",
                "        value_bck_colors=[\"#006B3F\"] * len(RADAR_LABELS),\n",
                "        blank_alpha=0.4,\n",
                "        kwargs_slices=dict(edgecolor=\"#000000\", zorder=2, linewidth=1),\n",
                "        kwargs_params=dict(color=\"#000000\", fontsize=12, va=\"center\"),\n",
                "        kwargs_values=dict(color=\"#ffffff\", fontsize=12, fontweight='bold',\n",
                "                          bbox=dict(edgecolor=\"#000000\", facecolor=\"#006B3F\",\n",
                "                                   boxstyle=\"round,pad=0.2\", lw=1))\n",
                "    )\n",
                "    \n",
                "    # Title\n",
                "    fig.text(0.515, 0.975, f\"{name}\", size=22, ha=\"center\", fontweight='bold', color=\"#006B3F\")\n",
                "    fig.text(0.515, 0.945, f\"{team} | {pos}\", size=14, ha=\"center\", color=\"#666666\")\n",
                "    fig.text(0.515, 0.025, \"Percentile Rank vs All Players | Data: FBref\",\n",
                "             size=10, ha=\"center\", color=\"#666666\")\n",
                "    \n",
                "    if save_path:\n",
                "        fig.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='#f4f4f4')\n",
                "    \n",
                "    plt.show()\n",
                "    \n",
                "    return {'name': name, 'team': team, 'position': pos, 'percentiles': percentiles, 'labels': RADAR_LABELS}\n",
                "\n",
                "print(\"pizza_chart() ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_replacement(name: str, n: int = 10, max_age: Optional[int] = None, \n",
                "                     position: Optional[str] = None):\n",
                "    \"\"\"Find and display player replacements.\"\"\"\n",
                "    res = finder.find(name, n=n, max_age=max_age, position=position)\n",
                "    if res is not None:\n",
                "        cols = [PLAYER_COL, TEAM_COL, 'similarity']\n",
                "        if POS_COL in res.columns: cols.insert(2, POS_COL)\n",
                "        if AGE_COL in res.columns: cols.insert(3, AGE_COL)\n",
                "        display(res[cols].reset_index(drop=True))\n",
                "    return res"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def explain_similarity(player1: str, player2: str, top_n: int = 10):\n",
                "    \"\"\"Explain why two players are similar.\"\"\"\n",
                "    idx1 = finder._get_idx(player1)\n",
                "    idx2 = finder._get_idx(player2)\n",
                "    \n",
                "    if idx1 is None or idx2 is None:\n",
                "        print(\"Player not found\")\n",
                "        return None\n",
                "    \n",
                "    name1 = df.iloc[idx1][PLAYER_COL]\n",
                "    name2 = df.iloc[idx2][PLAYER_COL]\n",
                "    pos1 = df.iloc[idx1][POS_COL] if POS_COL else ''\n",
                "    pos2 = df.iloc[idx2][POS_COL] if POS_COL else ''\n",
                "    similarity = finder.sim[idx1, idx2]\n",
                "    \n",
                "    v1 = df_per90.iloc[idx1][FEATURES].values\n",
                "    v2 = df_per90.iloc[idx2][FEATURES].values\n",
                "    v1_scaled = X_scaled[idx1]\n",
                "    v2_scaled = X_scaled[idx2]\n",
                "    \n",
                "    norm1 = np.linalg.norm(v1_scaled)\n",
                "    norm2 = np.linalg.norm(v2_scaled)\n",
                "    contributions = (v1_scaled * v2_scaled) / (norm1 * norm2 + 1e-8)\n",
                "    \n",
                "    analysis = pd.DataFrame({\n",
                "        'Feature': [get_readable_name(f) for f in FEATURES],\n",
                "        'Raw': FEATURES,\n",
                "        name1: v1,\n",
                "        name2: v2,\n",
                "        'Contribution': contributions\n",
                "    })\n",
                "    \n",
                "    print(\"=\" * 70)\n",
                "    print(f\"{name1} ({pos1}) vs {name2} ({pos2})\")\n",
                "    print(f\"Overall Similarity: {similarity:.1%}\")\n",
                "    print(\"=\" * 70)\n",
                "    print(f\"\\nTop {top_n} matching features:\")\n",
                "    for _, row in analysis.nlargest(top_n, 'Contribution').iterrows():\n",
                "        print(f\"  {row['Feature']:30s} | {row[name1]:6.2f} vs {row[name2]:6.2f} | +{row['Contribution']:.3f}\")\n",
                "    \n",
                "    return analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Example Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# find_replacement(\"Semenyo\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# radar_chart(\"Semenyo\", \"Elanga\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# pizza_chart(\"Kudus\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# explain_similarity(\"Semenyo\", \"Elanga\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Export for Web App"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save embeddings\n",
                "df_embeddings = df.copy()\n",
                "for i in range(EMB_DIM):\n",
                "    df_embeddings[f'emb_{i}'] = embeddings[:, i]\n",
                "df_embeddings.to_csv(DATA_DIR / f\"players_{SOURCE}_embeddings.csv\", index=False)\n",
                "\n",
                "# Save finder pickle\n",
                "with open(MODELS_DIR / 'finder_allpos.pkl', 'wb') as f:\n",
                "    pickle.dump({\n",
                "        'model': model.state_dict(),\n",
                "        'scaler': scaler,\n",
                "        'features': FEATURES,\n",
                "        'feature_names': FEATURE_NAMES,\n",
                "        'embeddings': embeddings,\n",
                "        'similarity': finder.sim,\n",
                "        'source': SOURCE,\n",
                "        'seed': SEED,\n",
                "        'emb_dim': EMB_DIM,\n",
                "        'radar_features': RADAR_FEATURES,\n",
                "        'radar_labels': RADAR_LABELS,\n",
                "        'radar_low': RADAR_LOW,\n",
                "        'radar_high': RADAR_HIGH\n",
                "    }, f)\n",
                "\n",
                "print(\"Saved model and embeddings!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def export_for_supabase():\n",
                "    \"\"\"Export data in format ready for Supabase import.\"\"\"\n",
                "    \n",
                "    # Players table\n",
                "    players = []\n",
                "    for i, row in df.iterrows():\n",
                "        players.append({\n",
                "            'id': i,\n",
                "            'name': row[PLAYER_COL],\n",
                "            'squad': row[TEAM_COL],\n",
                "            'position': row[POS_COL] if POS_COL else None,\n",
                "            'age': int(row[AGE_COL]) if AGE_COL and pd.notna(row[AGE_COL]) else None,\n",
                "            'nation': row.get('Nation', None),\n",
                "            'league': row.get('League', None),\n",
                "        })\n",
                "    \n",
                "    # Stats (per-90 values for radar)\n",
                "    stats = []\n",
                "    for i, row in df_per90.iterrows():\n",
                "        for j, feat in enumerate(RADAR_FEATURES):\n",
                "            stats.append({\n",
                "                'player_id': i,\n",
                "                'stat_name': RADAR_LABELS[j],\n",
                "                'stat_key': feat,\n",
                "                'value': round(float(row[feat]), 3) if pd.notna(row[feat]) else 0,\n",
                "                'min_range': round(RADAR_LOW[j], 3),\n",
                "                'max_range': round(RADAR_HIGH[j], 3)\n",
                "            })\n",
                "    \n",
                "    # Similarities (top 50 per player)\n",
                "    similarities = []\n",
                "    for i in range(len(df)):\n",
                "        sim_scores = finder.sim[i]\n",
                "        top_indices = np.argsort(sim_scores)[-51:-1][::-1]\n",
                "        for rank, j in enumerate(top_indices, 1):\n",
                "            similarities.append({\n",
                "                'player_id': i,\n",
                "                'similar_player_id': int(j),\n",
                "                'similarity': round(float(sim_scores[j]), 4),\n",
                "                'rank': rank\n",
                "            })\n",
                "    \n",
                "    # Save to JSON\n",
                "    with open(DATA_DIR / 'export_players.json', 'w') as f:\n",
                "        json.dump(players, f)\n",
                "    with open(DATA_DIR / 'export_stats.json', 'w') as f:\n",
                "        json.dump(stats, f)\n",
                "    with open(DATA_DIR / 'export_similarities.json', 'w') as f:\n",
                "        json.dump(similarities, f)\n",
                "    \n",
                "    print(f\"Exported:\")\n",
                "    print(f\"  - {len(players)} players\")\n",
                "    print(f\"  - {len(stats)} stat records\")\n",
                "    print(f\"  - {len(similarities)} similarity records\")\n",
                "    \n",
                "# export_for_supabase()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}